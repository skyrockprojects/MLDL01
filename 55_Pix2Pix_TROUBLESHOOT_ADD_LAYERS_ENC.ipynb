{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "55. Pix2Pix",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/J-Fo-S/ml-summer/blob/master/55_Pix2Pix_TROUBLESHOOT_ADD_LAYERS_ENC.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "a-FiDWfJcL6h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np,sys,os\n",
        "from numpy import float32\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.ndimage import imread\n",
        "from scipy.misc import imresize\n",
        "\n",
        "np.random.seed(678)\n",
        "tf.set_random_seed(678)\n",
        "\n",
        "# Activation Functions - however there was no indication in the original paper\n",
        "def tf_Relu(x): return tf.nn.relu(x)\n",
        "def d_tf_Relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
        "def tf_LRelu(x): return tf.nn.leaky_relu(x)\n",
        "def d_tf_LRelu(x): return tf.cast(tf.greater(x,0),tf.float32) + tf.cast(tf.less_equal(x,0),tf.float32) *x* 0.2\n",
        "def tf_tanh(x): return tf.nn.tanh(x)\n",
        "def d_tf_tanh(x): return 1.0 - tf.square(tf_tanh(x))\n",
        "def tf_log(x): return tf.nn.sigmoid(x)\n",
        "def d_tf_log(x): return tf_log(x) * (1-tf_log(x))\n",
        "\n",
        "# convolution layer\n",
        "class CNNLayer():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c,act,d_act):\n",
        "        self.w = tf.Variable(tf.truncated_normal([ker,ker,in_c,out_c],stddev=0.02))\n",
        "        self.act,self.d_act = act,d_act\n",
        "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "    def getw(self): return [self.w]\n",
        "    def feedforward(self,input,stride=2,batch_norm=True,padding_val='SAME'):\n",
        "        self.input  = input\n",
        "        self.layer  = tf.nn.conv2d(input,self.w,strides = [1,stride,stride,1],padding=padding_val)\n",
        "        if batch_norm: self.layer = tf.nn.batch_normalization(self.layer,mean=0.0,variance=1.0,variance_epsilon=1e-8,scale=True,offset=True)\n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "    def backprop(self,gradient,stride=1):\n",
        "        grad_part_1 = gradient\n",
        "        grad_part_2 = self.d_act(self.layer)\n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = tf.multiply(grad_part_1,grad_part_2)\n",
        "        grad = tf.nn.conv2d_backprop_filter(\n",
        "            input = grad_part_3,filter_sizes = self.w.shape,\n",
        "            out_backprop = grad_middle,strides=[1,1,1,1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        grad_pass  = tf.nn.conv2d_backprop_input(\n",
        "            input_sizes=[batch_size] + list(self.input.shape[1:]),filter = self.w ,\n",
        "            out_backprop = grad_middle,strides=[1,1,1,1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        update_w = []\n",
        "\n",
        "        update_w.append(\n",
        "            tf.assign( self.m,self.m*beta_1 + (1-beta_1) * grad   )\n",
        "        )\n",
        "        update_w.append(\n",
        "            tf.assign( self.v,self.v*beta_2 + (1-beta_2) * grad ** 2   )\n",
        "        )\n",
        "\n",
        "        m_hat = self.m / (1-beta1)\n",
        "        v_hat = self.v / (1-beta2)\n",
        "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat))))\n",
        "\n",
        "        return grad_pass,update_w\n",
        "\n",
        "class CNNLayer_Up():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c,act,d_act):\n",
        "        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.02))\n",
        "        self.act,self.d_act = act,d_act\n",
        "    def getw(self): return [self.w]\n",
        "    def feedforward(self,input,stride=1,dilate=1,output=1,batch_norm=True,dropout=False):\n",
        "        self.input  = input\n",
        "        current_shape_size = input.shape\n",
        "        self.layer = tf.nn.conv2d_transpose(self.input,self.w,\n",
        "        output_shape=[batch_size] + [int(current_shape_size[1].value * 2 ),int(current_shape_size[2].value*2 ),int(current_shape_size[3].value/int(self.w.shape[3].value/self.w.shape[2].value )  )],\n",
        "        strides=[1,2,2,1],padding='SAME')\n",
        "        if batch_norm: self.layer = tf.nn.batch_normalization(self.layer,mean=0.0,variance=1.0,variance_epsilon=1e-8,scale=True,offset=True)\n",
        "        if dropout: self.layer = tf.nn.dropout( self.layer,0.5)\n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "# data\n",
        "data_location = \"../../Dataset/salObj/datasets/imgs/pascal/\"\n",
        "train_data = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".jpg\" in filename.lower() :\n",
        "            train_data.append(os.path.join(dirName,filename))\n",
        "\n",
        "data_location =  \"../../Dataset/salObj/datasets/masks/pascal/\"\n",
        "train_data_gt = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".png\" in filename.lower() :\n",
        "            train_data_gt.append(os.path.join(dirName,filename))\n",
        "\n",
        "data_location =  \"../../Dataset/Semanticdataset100/image/\"\n",
        "test_data_c = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".jpg\" in filename.lower() :\n",
        "            test_data_c.append(os.path.join(dirName,filename))\n",
        "\n",
        "data_location =  \"../../Dataset/Semanticdataset100/ground-truth/\"\n",
        "test_data = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".png\" in filename.lower() :\n",
        "            test_data.append(os.path.join(dirName,filename))\n",
        "\n",
        "train_images = np.zeros(shape=(850,256,256,3))\n",
        "train_labels = np.zeros(shape=(850,256,256,1))\n",
        "test_images_c = np.zeros(shape=(850,256,256,3))\n",
        "test_images = np.zeros(shape=(100,256,256,1))\n",
        "\n",
        "for file_index in range(len(train_data)-1):\n",
        "    train_images[file_index,:,:]   = imresize(imread(train_data[file_index],mode='RGB'),(256,256))\n",
        "    train_labels[file_index,:,:]   = np.expand_dims(imresize(imread(train_data_gt[file_index],mode='F',flatten=True),(256,256)),axis=3)\n",
        "for file_index in range(len(test_data)):\n",
        "    test_images_c[file_index,:,:]   = imresize(imread(test_data_c[file_index],mode='RGB'),(256,256))\n",
        "    test_images[file_index,:,:]   = np.expand_dims(imresize(imread(test_data[file_index],mode='F',flatten=True),(256,256)),axis=3)\n",
        "\n",
        "train_images[:,:,:,0]  = (train_images[:,:,:,0] - train_images[:,:,:,0].min(axis=0)) / (train_images[:,:,:,0].max(axis=0) - train_images[:,:,:,0].min(axis=0)+ 1e-10)\n",
        "train_images[:,:,:,1]  = (train_images[:,:,:,1] - train_images[:,:,:,1].min(axis=0)) / (train_images[:,:,:,1].max(axis=0) - train_images[:,:,:,1].min(axis=0)+ 1e-10)\n",
        "train_images[:,:,:,2]  = (train_images[:,:,:,2] - train_images[:,:,:,2].min(axis=0)) / (train_images[:,:,:,2].max(axis=0) - train_images[:,:,:,2].min(axis=0)+ 1e-10)\n",
        "train_labels[:,:,:,0]  = (train_labels[:,:,:,0] - train_labels[:,:,:,0].min(axis=0)) / (train_labels[:,:,:,0].max(axis=0) - train_labels[:,:,:,0].min(axis=0)+ 1e-10)\n",
        "\n",
        "test_images_c[:,:,:,0]  = (test_images_c[:,:,:,0] - test_images_c[:,:,:,0].min(axis=0)) / (test_images_c[:,:,:,0].max(axis=0) - test_images_c[:,:,:,0].min(axis=0)+ 1e-10)\n",
        "test_images_c[:,:,:,1]  = (test_images_c[:,:,:,1] - test_images_c[:,:,:,1].min(axis=0)) / (test_images_c[:,:,:,1].max(axis=0) - test_images_c[:,:,:,1].min(axis=0)+ 1e-10)\n",
        "test_images_c[:,:,:,2]  = (test_images_c[:,:,:,2] - test_images_c[:,:,:,2].min(axis=0)) / (test_images_c[:,:,:,2].max(axis=0) - test_images_c[:,:,:,2].min(axis=0)+ 1e-10)\n",
        "test_images[:,:,:,0]  = (test_images[:,:,:,0] - test_images[:,:,:,0].min(axis=0)) / (test_images[:,:,:,0].max(axis=0) - test_images[:,:,:,0].min(axis=0)+ 1e-10)\n",
        "\n",
        "train_data = train_labels\n",
        "train_gt =   train_images\n",
        "\n",
        "# hyper\n",
        "num_epoch = 800\n",
        "learning_rate = 0.00001\n",
        "batch_size = 2\n",
        "print_size = 2\n",
        "\n",
        "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
        "\n",
        "# generator ---------\n",
        "# encoder (unet)\n",
        "gl1 = CNNLayer(4,1,128,tf_LRelu,d_tf_LRelu)\n",
        "gl2 = CNNLayer(4,128,256,tf_LRelu,d_tf_LRelu)\n",
        "gl3 = CNNLayer(4,256,512,tf_LRelu,d_tf_LRelu)\n",
        "gl4 = CNNLayer(4,512,512,tf_LRelu,d_tf_LRelu)\n",
        "gl5 = CNNLayer(4,512,512,tf_LRelu,d_tf_LRelu)\n",
        "\n",
        "# decoder\n",
        "gl6 = CNNLayer_Up(4,512,512,tf_Relu,d_tf_Relu)\n",
        "gl7 = CNNLayer_Up(4,512,1024,tf_Relu,d_tf_Relu)\n",
        "gl8 = CNNLayer_Up(4,256,1024,tf_Relu,d_tf_Relu)\n",
        "gl9 = CNNLayer_Up(4,128,512,tf_Relu,d_tf_Relu)\n",
        "gl10 = CNNLayer_Up(4,128,256,tf_Relu,d_tf_Relu)\n",
        "glfinal = CNNLayer(4,128,3,tf_tanh,d_tf_tanh)\n",
        "generator_w = gl1.getw() + gl2.getw() + gl3.getw() + gl4.getw() + gl5.getw() + \\\n",
        "gl6.getw() + gl7.getw() + gl8.getw() + gl9.getw() + gl10.getw() + glfinal.getw()\n",
        "# generator ---------\n",
        "\n",
        "# discrimator -------\n",
        "dl1 = CNNLayer(4,4,64,tf_LRelu,d_tf_LRelu)\n",
        "dl2 = CNNLayer(4,64,128,tf_LRelu,d_tf_LRelu)\n",
        "dl3 = CNNLayer(4,128,256,tf_LRelu,d_tf_LRelu)\n",
        "dl4 = CNNLayer(4,256,512,tf_LRelu,d_tf_LRelu)\n",
        "dlfinal = CNNLayer(4,512,1,tf_log,d_tf_log)\n",
        "discrimator_w = dl1.getw() + dl2.getw() + dl3.getw() + dl4.getw() + dlfinal.getw() \n",
        "\n",
        "# graph\n",
        "input_binary_image = tf.placeholder(shape=[None,256,256,1],dtype=tf.float32)\n",
        "color_image = tf.placeholder(shape=[None,256,256,3],dtype=tf.float32)\n",
        "\n",
        "g_e_layer1 = gl1.feedforward(input_binary_image,batch_norm=False)\n",
        "g_e_layer2 = gl2.feedforward(g_e_layer1)\n",
        "g_e_layer3 = gl3.feedforward(g_e_layer2)\n",
        "g_e_layer4 = gl4.feedforward(g_e_layer3)\n",
        "g_e_layer5 = gl5.feedforward(g_e_layer4)\n",
        "\n",
        "g_e_layer6 = gl6.feedforward(g_e_layer5)\n",
        "g_e_layer7 = gl7.feedforward(tf.concat([g_e_layer6,g_e_layer4],axis=3))\n",
        "g_e_layer8 = gl8.feedforward(tf.concat([g_e_layer7,g_e_layer3],axis=3))\n",
        "g_e_layer9 = gl9.feedforward(tf.concat([g_e_layer8,g_e_layer2],axis=3))\n",
        "g_e_layer10 = gl10.feedforward(tf.concat([g_e_layer9,g_e_layer1],axis=3))\n",
        "g_e_layer_final= glfinal.feedforward(g_e_layer10,stride=1)\n",
        "\n",
        "# -------- discriminator\n",
        "true_discrim_input = tf.concat([input_binary_image,color_image],axis=3)\n",
        "fake_discrim_input = tf.concat([input_binary_image,g_e_layer_final],axis=3)\n",
        "\n",
        "true_d_1 = dl1.feedforward(true_discrim_input)\n",
        "true_d_2 = dl2.feedforward(true_d_1)\n",
        "true_d_3 = dl3.feedforward(true_d_2)\n",
        "true_d_4 = dl4.feedforward(true_d_3)\n",
        "true_d_f = dlfinal.feedforward(true_d_4)\n",
        "\n",
        "fake_d_1 = dl1.feedforward(fake_discrim_input)\n",
        "fake_d_2 = dl2.feedforward(fake_d_1)\n",
        "fake_d_3 = dl3.feedforward(fake_d_2)\n",
        "fake_d_4 = dl4.feedforward(fake_d_3)\n",
        "fake_d_f = dlfinal.feedforward(fake_d_4)\n",
        "\n",
        "# ----- losses\n",
        "g_loss = tf.reduce_mean(-tf.log(fake_d_f+1e-10)) + 100 * tf.reduce_mean(tf.abs(color_image - g_e_layer_final))\n",
        "d_loss = tf.reduce_mean(-tf.log(true_d_f+1e-10) + tf.log(1.0 - fake_d_f+1e-10))\n",
        "\n",
        "# --- training\n",
        "auto_g_train = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.5).minimize(g_loss,var_list=generator_w)\n",
        "auto_d_train = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.5).minimize(d_loss,var_list=discrimator_w)\n",
        "\n",
        "\n",
        "# session\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for iter in range(num_epoch):\n",
        "        \n",
        "        train_data,train_gt = shuffle(train_data,train_gt)\n",
        "        for current_batch_index in range(0,len(train_data),batch_size):\n",
        "            current_image_batch = train_data[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            current_mask_batch  = train_gt[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "\n",
        "            sess_results_d = sess.run([d_loss,auto_d_train],feed_dict={input_binary_image:current_image_batch,color_image:current_mask_batch})\n",
        "            sess_results_g = sess.run([g_loss,auto_g_train],feed_dict={input_binary_image:current_image_batch,color_image:current_mask_batch})\n",
        "            print(\"Current Iter: \",iter, \" current batch: \",current_batch_index,\" D Cost: \",sess_results_d[0], \" G Cost: \",sess_results_g[0],end='\\r')\n",
        "\n",
        "        if iter % print_size == 0:\n",
        "            print(\"\\n------------------------\\n\")\n",
        "            test_example =   train_data[:2,:,:,:]\n",
        "            test_example_gt = train_gt[:2,:,:,:]\n",
        "            sess_results = sess.run([g_e_layer_final],feed_dict={input_binary_image:test_example,color_image:test_example_gt})\n",
        "\n",
        "            sess_results = sess_results[0][0,:,:,:]\n",
        "            test_example = test_example[0,:,:,:]\n",
        "            test_example_gt = test_example_gt[0,:,:,:]\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(test_example),cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title('Original Mask ')\n",
        "            plt.savefig('train_change/'+str(iter)+\"a_Original_Mask.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(test_example_gt))\n",
        "            plt.axis('off')\n",
        "            plt.title('Ground Truth Image')\n",
        "            plt.savefig('train_change/'+str(iter)+\"b_Original_Image.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.axis('off')\n",
        "            plt.imshow(np.squeeze(sess_results)   ,cmap='gray')\n",
        "            plt.title(\"Generated Image\")\n",
        "            plt.savefig('train_change/'+str(iter)+\"e_Generated_Image.png\")\n",
        "\n",
        "            plt.close('all')       \n",
        "\n",
        "    # print halve test\n",
        "    for current_batch_index in range(0,len(test_images),batch_size):\n",
        "        test_example = test_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "        test_example_gt  = test_images_c[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "        sess_results = sess.run([g_e_layer_final],feed_dict={input_binary_image:test_example})\n",
        "\n",
        "        sess_results = sess_results[0][0,:,:,:]\n",
        "        test_example = test_example[0,:,:,:]\n",
        "        test_example_gt = test_example_gt[0,:,:,:]\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(np.squeeze(test_example),cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title('Original Mask ')\n",
        "        plt.savefig('gif/'+str(current_batch_index)+\"a_Original_Mask.png\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(np.squeeze(test_example_gt))\n",
        "        plt.axis('off')\n",
        "        plt.title('Ground Truth Image')\n",
        "        plt.savefig('gif/'+str(current_batch_index)+\"b_Original_Image.png\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.axis('off')\n",
        "        plt.imshow(np.squeeze(sess_results)   ,cmap='gray')\n",
        "        plt.title(\"Generated Image\")\n",
        "        plt.savefig('gif/'+str(current_batch_index)+\"e_Generated_Image.png\")\n",
        "\n",
        "        plt.close('all')       \n",
        "\n",
        "    # print halve train\n",
        "    for current_batch_index in range(0,len(train_data),batch_size):\n",
        "        test_example = train_data[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "        test_example_gt  = train_gt[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "        sess_results = sess.run([g_e_layer_final],feed_dict={input_binary_image:test_example})\n",
        "\n",
        "        sess_results = sess_results[0][0,:,:,:]\n",
        "        test_example = test_example[0,:,:,:]\n",
        "        test_example_gt = test_example_gt[0,:,:,:]\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(np.squeeze(test_example),cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title('Original Mask ')\n",
        "        plt.savefig('final/'+str(current_batch_index)+\"a_Original_Mask.png\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(np.squeeze(test_example_gt))\n",
        "        plt.axis('off')\n",
        "        plt.title('Ground Truth Image')\n",
        "        plt.savefig('final/'+str(current_batch_index)+\"b_Original_Image.png\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.axis('off')\n",
        "        plt.imshow(np.squeeze(sess_results)   ,cmap='gray')\n",
        "        plt.title(\"Generated Image\")\n",
        "        plt.savefig('final/'+str(current_batch_index)+\"e_Generated_Image.png\")\n",
        "\n",
        "        plt.close('all')       \n",
        "        \n",
        "\n",
        "# -- end code --"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}