{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Glove\" method of word2vec, adapted from PK Mital: https://github.com/pkmital/pycadl/blob/master/cadl/glove.py and Thrones2Vect by Yuriy Guts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from cadl import utils\n",
    "import zipfile\n",
    "from scipy.spatial import distance, distance_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.manifold\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Download the glove model and open a zip file\n",
    "file = utils.download('http://nlp.stanford.edu/data/wordvecs/glove.6B.zip')\n",
    "zf = zipfile.ZipFile(file)\n",
    "\n",
    "# Collect the words and their vectors\n",
    "words = []\n",
    "vectors = []\n",
    "for l in zf.open(\"glove.6B.300d.txt\"):\n",
    "    t = l.strip().split()\n",
    "    words.append(t[0].decode())\n",
    "    vectors.append(list(map(np.double, t[1:])))\n",
    "\n",
    "# Store as a lookup table\n",
    "wordvecs = np.asarray(vectors, dtype=np.double)\n",
    "word2id = {word: i for i, word in enumerate(words)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON THE NEXT ITERATION, we can use our own trained model - TRY LATER!\n",
    "TODO: Slice dim entries from top of txt files or there will be a sequence error when making the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile('trained/thrones2vec.zip')\n",
    "\n",
    "# Collect the words and their vectors\n",
    "words = []\n",
    "vectors = []\n",
    "for l in zf.open(\"thrones2vec.txt\"):\n",
    "    t = l.strip().split()\n",
    "    words.append(t[0].decode())\n",
    "    vectors.append(list(map(np.double, t[1:])))\n",
    "\n",
    "# Store as a lookup table\n",
    "wordvecs = np.asarray(vectors, dtype=np.double)\n",
    "word2id = {word: i for i, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word = 'zoo'\n",
    "print(word2id[word])\n",
    "print(wordvecs[word2id[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distances to target word\n",
    "target_vec = wordvecs[word2id[word]]\n",
    "dists = []\n",
    "for vec_i in wordvecs:\n",
    "    dists.append(distance.cosine(target_vec, vec_i))\n",
    "\n",
    "k = 20\n",
    "\n",
    "# Print top nearest words\n",
    "idxs = np.argsort(dists)\n",
    "for idx_i in idxs[:k]:\n",
    "    print(words[idx_i], dists[idx_i])\n",
    "\n",
    "# Plot top nearest words\n",
    "labels = [words[idx_i] for idx_i in idxs[:k]]\n",
    "plt.figure()\n",
    "plt.bar(range(k),\n",
    "        [dists[idx_i] for idx_i in idxs[:k]])\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation='vertical')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('distances')\n",
    "\n",
    "# Create distance matrix\n",
    "vecs = [wordvecs[idx_i] for idx_i in idxs[:k]]\n",
    "dm = distance_matrix(vecs, vecs)\n",
    "plt.figure()\n",
    "plt.imshow(dm)\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation='vertical')\n",
    "ax.set_yticklabels(labels)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot data points in reduced dimensionality using principal components\n",
    "# of the distance matrix\n",
    "res = PCA(2).fit_transform(dm / np.mean(dm, axis=0, keepdims=True))\n",
    "pc1, pc2 = res[:, 0], res[:, 1]\n",
    "plt.figure()\n",
    "plt.scatter(pc1, pc2)\n",
    "for i in range(len(labels)):\n",
    "    plt.text(pc1[i], pc2[i], labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a 'dataframe' to plot a lot more words and show their embedded relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create distance matrix to plot many word embeddings - will TAKE A BIT\n",
    "\n",
    "many_vec = 10000\n",
    "\n",
    "def pca_many(): \n",
    "    vecs = [wordvecs[idx_i] for idx_i in idxs[:many_vec]]\n",
    "    dm = distance_matrix(vecs, vecs)\n",
    "    res = PCA(2).fit_transform(dm / np.mean(dm, axis=0, keepdims=True))\n",
    "    return res\n",
    "\n",
    "res = pca_many()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot all words by using pandas dataframe\n",
    "word_arr = np.asarray(words[:many_vec])\n",
    "\n",
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (label, coords[0], coords[1])\n",
    "        for label, coords in [\n",
    "            (label,  res[word2id[label]])\n",
    "            for label in word_arr\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"label\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "points[200:215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom in on different regions and visualize word relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.label, fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some other regions - what can we infer?\n",
    "plot_region(x_bounds=(1., 2), y_bounds=(-2, -1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since our model actually contains some 400,000 words and it would take a long time to analyze all together, let's continue by analyzing any word but with only its k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_nearest_words(word, k=20):\n",
    "        \"\"\"Summary\n",
    "        Parameters\n",
    "        ----------\n",
    "        word : TYPE\n",
    "            Description\n",
    "        k : int, optional\n",
    "            Description\n",
    "        \"\"\"\n",
    "        # Get distances to target word\n",
    "        target_vec = wordvecs[word2id[word]]\n",
    "        dists = []\n",
    "        for vec_i in wordvecs:\n",
    "            dists.append(distance.cosine(target_vec, vec_i))\n",
    "        idxs = np.argsort(dists)\n",
    "        labels = [words[idx_i] for idx_i in idxs[:k]]\n",
    "        vecs = [wordvecs[idx_i] for idx_i in idxs[:k]]\n",
    "        dm = distance_matrix(vecs, vecs)\n",
    "        plt.figure()\n",
    "        plt.imshow(dm)\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "        # Create distance matrix\n",
    "        axs[0].imshow(dm)\n",
    "        axs[0].set_xticks(range(len(labels)))\n",
    "        axs[0].set_yticks(range(len(labels)))\n",
    "        axs[0].set_xticklabels(labels, rotation='vertical')\n",
    "        axs[0].set_yticklabels(labels)\n",
    "\n",
    "        # Center the distance matrix\n",
    "        dm = dm / np.mean(dm, axis=0, keepdims=True)\n",
    "\n",
    "        # Plot data points in reduced dimensionality using principal components\n",
    "        # of the distance matrix\n",
    "        res = PCA(2).fit_transform(dm)\n",
    "        pc1, pc2 = res[:, 0], res[:, 1]\n",
    "        axs[1].scatter(pc1, pc2)\n",
    "        for i in range(len(labels)):\n",
    "            axs[1].text(pc1[i], pc2[i], labels[i])\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nearest_words('2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nearest_words('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function which will return us the nearest words rather than\n",
    "# plot them:\n",
    "def get_nearest_words(target_vec, k=20):\n",
    "    \"\"\"Summary\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_vec : TYPE\n",
    "        Description\n",
    "    k : int, optional\n",
    "        Description\n",
    "    Returns\n",
    "    -------\n",
    "    TYPE\n",
    "        Description\n",
    "    \"\"\"\n",
    "    # Get distances to target vector\n",
    "    dists = []\n",
    "    for vec_i in wordvecs:\n",
    "        dists.append(distance.cosine(target_vec, vec_i))\n",
    "    # Get top nearest words\n",
    "    idxs = np.argsort(dists)\n",
    "    res = []\n",
    "    for idx_i in idxs[:k]:\n",
    "        res.append((words[idx_i], dists[idx_i]))\n",
    "    return res\n",
    "\n",
    "# And a convenience function for returning a vector\n",
    "def get_vector(word):\n",
    "    \"\"\"Summary\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : TYPE\n",
    "        Description\n",
    "    Returns\n",
    "    -------\n",
    "    TYPE\n",
    "        Description\n",
    "    \"\"\"\n",
    "    return wordvecs[word2id[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embeddings can sometimes show relationships with vector arithmetic\n",
    "get_nearest_words(get_vector('disease') - get_vector('death') + get_vector('hospital'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
